<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.148.2">
    <meta name="generator" content="Relearn 8.0.0+9803d5122ebb3276acea823f476e9eb44f607862">
    <meta name="description" content="Introduction acknowledgements
Goals Introduce probability theory and Bayesian inference from a set-based perspective. Uses real-world examples and examples to introduce key concepts, such as conditional and marginal probability, independence, and Bayes’ rule. Tailored examples to highlight common misconceptions (e.g., base-rate neglect with taxicab problem) and how thinking through it clearly using tools from tutorial. Provide the knowledge and confidence to start learning probabilistic computing and transition to that in the next tutorial Present everything in a friendly narrative for approachability Chibany is hungry">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="A Narrative Introduction to Probability :: Narrative Introduction to Probabilistic Computing">
    <meta name="twitter:description" content="Introduction acknowledgements
Goals Introduce probability theory and Bayesian inference from a set-based perspective. Uses real-world examples and examples to introduce key concepts, such as conditional and marginal probability, independence, and Bayes’ rule. Tailored examples to highlight common misconceptions (e.g., base-rate neglect with taxicab problem) and how thinking through it clearly using tools from tutorial. Provide the knowledge and confidence to start learning probabilistic computing and transition to that in the next tutorial Present everything in a friendly narrative for approachability Chibany is hungry">
    <meta property="og:url" content="http://localhost:1313/chapters/intro/index.html">
    <meta property="og:site_name" content="Narrative Introduction to Probabilistic Computing">
    <meta property="og:title" content="A Narrative Introduction to Probability :: Narrative Introduction to Probabilistic Computing">
    <meta property="og:description" content="Introduction acknowledgements
Goals Introduce probability theory and Bayesian inference from a set-based perspective. Uses real-world examples and examples to introduce key concepts, such as conditional and marginal probability, independence, and Bayes’ rule. Tailored examples to highlight common misconceptions (e.g., base-rate neglect with taxicab problem) and how thinking through it clearly using tools from tutorial. Provide the knowledge and confidence to start learning probabilistic computing and transition to that in the next tutorial Present everything in a friendly narrative for approachability Chibany is hungry">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="website">
    <meta itemprop="name" content="A Narrative Introduction to Probability :: Narrative Introduction to Probabilistic Computing">
    <meta itemprop="description" content="Introduction acknowledgements
Goals Introduce probability theory and Bayesian inference from a set-based perspective. Uses real-world examples and examples to introduce key concepts, such as conditional and marginal probability, independence, and Bayes’ rule. Tailored examples to highlight common misconceptions (e.g., base-rate neglect with taxicab problem) and how thinking through it clearly using tools from tutorial. Provide the knowledge and confidence to start learning probabilistic computing and transition to that in the next tutorial Present everything in a friendly narrative for approachability Chibany is hungry">
    <meta itemprop="datePublished" content="2025-09-25T00:00:00+00:00">
    <meta itemprop="dateModified" content="2025-09-25T00:00:00+00:00">
    <meta itemprop="wordCount" content="5299">
    <title>A Narrative Introduction to Probability :: Narrative Introduction to Probabilistic Computing</title>
    <link href="/chapters/intro/index.xml" rel="alternate" type="application/rss+xml" title="A Narrative Introduction to Probability :: Narrative Introduction to Probabilistic Computing">
    <link href="/images/favicon.png?1758795679" rel="icon" type="image/png">
    <link href="/css/auto-complete/auto-complete.min.css?1758795679" rel="stylesheet">
    <script src="/js/auto-complete/auto-complete.min.js?1758795679" defer></script>
    <script src="/js/search-lunr.js?1758795679" defer></script>
    <script src="/js/search.js?1758795679" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/searchindex.en.js?1758795679";
    </script>
    <script src="/js/lunr/lunr.min.js?1758795679" defer></script>
    <script src="/js/lunr/lunr.stemmer.support.min.js?1758795679" defer></script>
    <script src="/js/lunr/lunr.multi.min.js?1758795679" defer></script>
    <script src="/js/lunr/lunr.en.min.js?1758795679" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/fonts/fontawesome/css/fontawesome-all.min.css?1758795679" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/fonts/fontawesome/css/fontawesome-all.min.css?1758795679" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar/perfect-scrollbar.min.css?1758795679" rel="stylesheet">
    <link href="/css/theme.css?1758795679" rel="stylesheet">
    <link href="/css/format-html.css?1758795679" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = ``;
      window.relearn.path='\/chapters\/intro\/index.html';
      window.relearn.relBasePath='..\/..';
      window.relearn.relBaseUri='..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
  </head>
  <body class="mobile-support html" data-url="/chapters/intro/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#goals">Goals</a></li>
    <li><a href="#chibany-is-hungry">Chibany is hungry</a>
      <ul>
        <li><a href="#sets">Sets</a></li>
        <li><a href="#outcome-space">Outcome Space</a></li>
        <li><a href="#possibilities-vs-events">Possibilities vs. Events</a>
          <ul>
            <li><a href="#quick-check">Quick Check</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#probability-and-counting">Probability and Counting</a>
      <ul>
        <li><a href="#counting">Counting</a>
          <ul>
            <li><a href="#chibany-is-still-hungry-and-desires-tonkatsu">Chibany is still hungry&hellip; and desires Tonkatsu</a></li>
          </ul>
        </li>
        <li><a href="#probability-as-counting">Probability as Counting</a></li>
        <li><a href="#random-variables">Random Variables</a>
          <ul>
            <li><a href="#chibany-wants-to-know-how-much-tonkatsu">Chibany wants to know&hellip; how much Tonkatsu?</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#conditional-probability-as-changing-the-possible-outcomes">Conditional probability as changing the possible outcomes</a>
      <ul>
        <li><a href="#chibany-wants-a-tonkatsu-dinner">Chibany wants a tonkatsu dinner</a></li>
        <li><a href="#defining-conditional-probability-as-set-restriction">Defining conditional probability as set restriction</a></li>
        <li><a href="#dependence-and-independence">Dependence and independence</a></li>
        <li><a href="#marginal-and-joint-probabilities">Marginal and joint probabilities</a>
          <ul>
            <li><a href="#chibany-is-sad-marginalization">Chibany is sad (marginalization)</a></li>
            <li><a href="#the-sum-rule-more-on-marginalization-and-marginal-probabilities">The Sum Rule: More on Marginalization and Marginal Probabilities</a></li>
            <li><a href="#the-other-definition-of-conditional-probability">The other definition of conditional probability</a></li>
          </ul>
        </li>
        <li><a href="#weighted-possibilities">Weighted possibilities</a>
          <ul>
            <li><a href="#chibany-tells-students-that-he-likes-tonkatsu-more">Chibany tells students that he likes Tonkatsu more</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#bayes-theorem-or-bayes-rule">Bayes&rsquo; Theorem or Bayes&rsquo; Rule</a>
      <ul>
        <li><a href="#proving-bayes-rule">Proving Bayes&rsquo; rule</a></li>
        <li><a href="#the-taxicab-problem">The Taxicab Problem</a>
          <ul>
            <li><a href="#taxicab-solution-1">Taxicab Solution 1</a></li>
            <li><a href="#taxicab-solution-2">Taxicab Solution 2</a></li>
            <li><a href="#why-learn-the-set-based-perspective-to-probability-theory">Why Learn the Set-Based Perspective to Probability Theory?</a></li>
            <li><a href="#transfer-additional-practice-questions">Transfer additional practice questions</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#glossary">Glossary</a>
      <ul>
        <li>
          <ul>
            <li><a href="#set">set</a></li>
            <li><a href="#event">event</a></li>
            <li><a href="#cardinality">cardinality</a></li>
            <li><a href="#probability">probability</a></li>
            <li><a href="#random-variable">random variable</a></li>
            <li><a href="#conditional-probability">conditional probability</a></li>
            <li><a href="#dependence">dependence</a></li>
            <li><a href="#marginal-probability">marginal probability</a></li>
            <li><a href="#joint-probability">joint probability</a></li>
            <li><a href="#bayes-theorem">Bayes theorem</a></li>
            <li><a href="#generative-process">generative process</a></li>
            <li><a href="#probabilistic-computing">probabilistic computing</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/index.html"><span itemprop="name">Narrative Introduction to Probabilistic Computing</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/chapters/index.html"><span itemprop="name">Chapters</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">A Narrative Introduction to Probability</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/chapters/index.html" title="Chapters (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/chapters/intro2/index.html" title="Intro2 (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable chapters" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="a-narrative-introduction-to-probability">A Narrative Introduction to Probability</h1>

<h1 id="introduction">Introduction</h1>
<p><a href="/chapters/intro/index.html#acknowledgements">acknowledgements</a></p>
<h1 id="goals">Goals</h1>
<ol>
<li>Introduce probability theory and Bayesian inference from a set-based perspective.</li>
<li>Uses real-world examples and examples to introduce key concepts, such as conditional and marginal probability, independence, and Bayes’ rule.</li>
<li>Tailored examples to highlight common misconceptions (e.g., <a href="https://en.wikipedia.org/wiki/Representativeness_heuristic#The_taxicab_problem" rel="external" target="_blank">base-rate neglect with taxicab problem</a>) and how thinking through it clearly using tools from tutorial.</li>
<li>Provide the knowledge and confidence to start learning probabilistic computing and transition to that in the next tutorial</li>
<li>Present everything in a friendly narrative for approachability</li>
</ol>
<h1 id="chibany-is-hungry">Chibany is hungry</h1>
<p><a href="#R-image-7451515fb3b240df86fde10fe17a307e" class="lightbox-link"><img alt="chibany laying down" class="lazy lightbox figure-image" loading="lazy" src="/images/chibanylayingdown.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7451515fb3b240df86fde10fe17a307e"><img alt="chibany laying down" class="lazy lightbox lightbox-image" loading="lazy" src="/images/chibanylayingdown.png"></a></p>
<p>Chibany wakes up from dreaming of the delicious meals he will get later today. Twice per day, a student brings a bento box with a meal as an offering to Chibany. One student brings him a bento box in the early afternoon for lunch and a different student brings him a bento box in the evening for dinner. The meal is either a Hamburger <i class="fa-fw fas fa-burger" style="color: white;"></i>
or a Tonkatsu (pork cutlet) <i class="fa-fw fas fa-piggy-bank" style="color: white;"></i>. To keep track of his meal possibilities, his lists out the four possibilities:</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 2
        a[&#34;H(amburger) H(amburger)&#34;] b[&#34;H(amburger) T(onkastu)&#34;]
        c[&#34;T(onkastu) H(amburger)&#34;] d[&#34;T(onkastu) T(onkastu)&#34;]
    end</pre>
<h2 id="sets">Sets</h2>
<p>This forms a <a href="/chapters/intro/index.html#set">set</a> of four elements. A set is a collection of elements or members. In this case, an element is defined by the two meals given to Chibany that day. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and &ldquo;$\{$&rdquo; denotes the start of a set and &ldquo;$\}$&rdquo; the end of a set.</p>
<h2 id="outcome-space">Outcome Space</h2>
<p>In the context of probability theory, the basic elements of what can occur are called <em>outcomes</em>. Outcomes are the fundamental building blocks probabilities are from. As they are fundamental, the Greek letter $\Omega$ is frequently used to refer to this set of possibile <em>outcomes</em>. Diligently noting his daily offerings, Chibany defines $\Omega = \{HH, HT, TH, TT \} $. The first letter defines his lunch offering, and the second letter defines his dinner offering. He notes that $H$ now always refers to hamburgers and $T$ to tonkatsu.</p>
<p>Note that technically, the elements of a set are unique. So, if Chibany writes down getting a pair of hamburgers twice and a hamburger and a tonkatsu ($\{HH, HH, HT\}$), he&rsquo;s gotten the same set of possibilities as if he only got one pair of hamburgers and a hamburger and tonkatsu ($\{HH, HT\}$). In other words, $\{HH, HH, HT\} = \{HH, HT\}$.</p>
<p>Chibany is skeptical, but will try to keep it in mind. It can be confusing!</p>
<h2 id="possibilities-vs-events">Possibilities vs. Events</h2>
<p>So far, we have discussed sets, possibile outcomes and the set of all possible outcomes $\Omega$. Chibany is interested in the set of possible meals that include Tonkatsu. What is this set?</p>
<p><em>TODO</em>: Add interactive elements to quiz reader as they go</p>
<p>$\{HT, TH, TT\}$</p>
<p>This is an example of an <a href="/chapters/intro/index.html#event">event</a>. Technically, an event or a set that is none, some, or all of the possible outcomes.</p>
<h3 id="quick-check">Quick Check</h3>
<p>Is $\Omega$ an event?</p>
<p>Yes &ndash; it is the event that contains all possible outcomes.</p>
<p>Is $\Omega$ the set of all possible events?</p>
<p>No.</p>
<p>What is the set of all possible events for Chibany&rsquo;s situation?</p>
<p>$\{ \{ \}, \{ HH \}, \{ HT\}, \{TH \}, \{TT\},
\{HH,HT\}, \{HH,TH\}, \{HH,TT\},
\{HT, TH\}, \{HT, TT \},
\{TH, TT\}
\{HH, HT, TH\}, \{HH, HT, TT \}, \{HH, TH, TT\},
\{HT, TH, TT\},
\{HH, HT, TH, TT\}  \}$</p>
<p>Note that $\{ \}$ is called the empty or null set and is a special set that contains no elements.</p>
<h1 id="probability-and-counting">Probability and Counting</h1>
<p>One goal of this tutorial is to show you that probability is counting. When every possibility is equally likely, probability is defined as the relative number of possibilities in each set. When possibilities are not equally likely, it is only slightly more complicated. Rather than each possibility counting one towards the size of a set it is in, you count the possibility according to its relative weight.</p>
<h2 id="counting">Counting</h2>
<p>The basic operation that we use to define probabilities is counting the number of elements in a set. If $A$ is a set, then $|A|$ is the <a href="/chapters/intro/index.html#cardinality">cardinality</a> or size of the set. For example, the set of Chibany&rsquo;s lunch options is $\{H,T\}$. Counting the number of elements determines its size, which is $\left|\{H, T\} \right| = 2$. The set of Chibany&rsquo;s meal offerings for a day, $\Omega = \{HH, HT, TH, TT \}$. There are four possibilities, so its size $|\Omega|$ is $ 4$.</p>
<h3 id="chibany-is-still-hungry-and-desires-tonkatsu">Chibany is still hungry&hellip; and desires Tonkatsu</h3>
<p>Chibany is still hungry and wondering what his meal possibilities are for the day. He wonders, what is the probability that students appease him today by giving him Tonkatsu?</p>
<p>To make this calculation, Chibany lists out the outcome space $\Omega$ again. He then forms the event &ldquo;Tonkatsu offering today&rdquo;. He defines the set of possible outcomes with a Tonkatsu as $A = \{HT, TH, TT\}$ to encode the event. He highlights those in red. Chibany thinks &ldquo;wow&hellip; three of the four possible outcomes are red. Fortune must favor me today, right?&rdquo;</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 2
        a[&#34;H(amburger) H(amburger)&#34;] b[&#34;H(amburger) T(onkastu)&#34;]
        c[&#34;T(onkastu) H(amburger)&#34;] d[&#34;T(onkastu) T(onkastu)&#34;]
    end
    style b stroke: #f33, stroke-width:4px
    style c stroke: #f33, stroke-width:4px
    style d stroke: #f33, stroke-width:4px</pre>
<p>Yes, Chibany, it does as it always should. Your chance of getting Tonkatsu is three out of four or 0.75. He calculated the probability exactly as he should!</p>
<h2 id="probability-as-counting">Probability as Counting</h2>
<p>The <a href="/chapters/intro/index.html#probability">probability</a> of an event $A$ is $\frac{|A|}{|\Omega|}$. It is written as $P(A)$. In the prior example, $|A| = | \{HT, TH, TT\} |$ and $|\Omega| = | \{HH, HT, TH, TT\}|$ have three and four elements, respectively. Note that if the possible outcomes were not equally likely, we would sum their individual probabilities to calculate the cardinality. But everything works in the same way &ndash; the probability of the event is the total &ldquo;size&rdquo; or &ldquo;weight&rdquo; of the possible outcomes in the event as compared to the total size or weight of all possible outcomes.</p>
<p>What is the probability that Chibany gets Tonkatsu for his first offering? Well the possible outcomes with Tonkatsu are $\{TH, TT\}$. There are four possible outcomes for his offerings $\Omega = \{HH,HT, TH, TT\}$. So the probability he gets Tonkatsu for his first offering is $|\{TH, TT\}|/|\{HH,HT, TH, TT\}| = 2/4=1/2$. Chibany draws the following table to illustrate his counting:</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 2
        a[&#34;H(amburger) H(amburger)&#34;] b[&#34;H(amburger) T(onkastu)&#34;]
        c[&#34;T(onkastu) H(amburger)&#34;] d[&#34;T(onkastu) T(onkastu)&#34;]
    end
    style c stroke: #f33, stroke-width:4px
    style d stroke: #f33, stroke-width:4px</pre>
<h2 id="random-variables">Random Variables</h2>
<h3 id="chibany-wants-to-know-how-much-tonkatsu">Chibany wants to know&hellip; how much Tonkatsu?</h3>
<p>Chibany wants to know how much Tonkatsu he gets each day. To do so, he converts each possibility to a whole number: the number of Tonkatsu in that possibility. He calls this a function $f : \Omega \rightarrow \{0, 1, 2, \ldots\}$, meaning it takes a possibility out of the outcome space and maps it (changes it into) a number. He notes: mapping every possibility to a whole number is like making each whole number an event! His Tonkatsu counter $f$ is defined as $f(HH) = 0$, $f(HT) = 1$, $f(TH)=1$, and $f(TT) = 2$. Chibany defined his first <a href="/chapters/intro/index.html#random-variable">random variable</a>.</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 2
        a[&#34;HH: 0&#34;] space
        b[&#34;HT: 1&#34;] c[&#34;TH: 1&#34;]
        d[&#34;TT: 2&#34;] space
    end
    style b stroke: #44c, stroke-width:4px
    style c stroke: #44c, stroke-width:4px
    style d stroke: #f33, stroke-width:4px</pre>
<p>What is the probability of having two tonkatsus? We count the number of outcomes with two tonkatsus ($\{TT\}$ highlighted in red) and divide by the number of possible outcomes ($|\Omega|=4$). So, it is 1 out of 4 or 1/4.</p>
<p>What about the probability of having <em>exactly</em> one tonkatsu? We count the number of outcomes with <em>exactly</em> one tonkatsu ($\{HT, TH\}$) and divide by the number of possible outcomes ($|\Omega|=4$). So it is 2/4 or 1/2.</p>
<h1 id="conditional-probability-as-changing-the-possible-outcomes">Conditional probability as changing the possible outcomes</h1>
<h2 id="chibany-wants-a-tonkatsu-dinner">Chibany wants a tonkatsu dinner</h2>
<p>A graduate of Chiba Tech, Tanaka-san, visits Chibany one day and tells Chinbany that he knows that there will be at least one tonkatsu in tomorrow&rsquo;s offering. Chibany is excited. They wants to know how likely it is that the second meal is a Tonkatsu. They quiz Tanaka-san. He say it&rsquo;s just as likely as before, so it should be 1/2. Chibany disagrees. Chibany says &ldquo;I learned something because I knows I will get at least one tonkatsu&rdquo;. Also, Chibany is an optimist and deserves to have all the tonkatsu. Who&rsquo;s right!? Let&rsquo;s check the chart&hellip;</p>
<p><strong>TODO: Make top block look more disabled</strong></p>
<pre class="mermaid align-center ">block-beta
    block
        columns 1
        a[&#34;HH&#34;] 
        block:group1
            b[&#34;HT&#34;] c[&#34;TH&#34;]
            d[&#34;TT&#34;] space
        end
    end
    style a fill:#999, text-decoration:line-through
    style group1 stroke: #33f, stroke-width: 6px
    style c stroke: #f33, stroke-width: 4px
    style d stroke: #f33, stroke-width: 4px</pre>
<p>In the case where there is at least one tonkatsu, the space of possible outcomes is $\{HT, TH, TT\}$, which is outlined in blue. The event of interest for Chibany is outlined in red. It turns out Chibany is correct! There is a two in three chance that he gets a tonkatsu dinner. That&rsquo;s larger than one in two.</p>
<p>Chibany kindly reminds Tanaka-san that you never stop learning and to consider taking one of Joe&rsquo;s classes at Chiba Tech. Chibany hears great things about them!</p>
<h2 id="defining-conditional-probability-as-set-restriction">Defining conditional probability as set restriction</h2>
<p>What Chibany calculated is a <a href="/chapters/intro/index.html#conditional-probability">conditional probability</a> &ndash; the probability of an event (two tonkatsus) conditioned on knowledge of another event (at least one tonkatsu). Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that <em>restricted</em> outcome space. In our example, we&rsquo;re interested in the probability of the event $A= \{TT\}$ conditioned on the knowledge that there&rsquo;s at least one tonkatsu, $ B = \Omega_{\geq 1 T}= \{HT, TH, TT\}$. Formally, this is written as $P(A \mid B) = \frac{|A|}{|B|}$, where everything to the left of the $\mid$ is what we&rsquo;re interested in knowing the probability of and everything to the right of the $\mid$ is what we know to be true.</p>
<p>Note that this is a different, yet equivalent perspective to how conditional probability is traditionally taught.</p>
<h2 id="dependence-and-independence">Dependence and independence</h2>
<p>Tanaka-san explains to Chibany his reasoning: He did not think whether Chibany received a tonkatsu (T) for their first offering influenced whether they receive a tonkatsu (T) for their second offering.</p>
<p>Chibany is curious. Tanaka-san&rsquo;s logic seems sound, but it sounds like a slightly different question. Chibany asks Tanaka-san to draw out the outcome space and events for this question to help clarify what is different. Tanaka-san states his question formally: What is the probability of getting a second tonkatsu ($\{TT\}$) given the first offering was a tonkatsu ($\{TH, TT\}$)  or $P(\{HT, TT\} \mid \{TH, TT\})$</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 1
        block:group1
            a[&#34;HH&#34;] b[&#34;HT&#34;]
        end
        block:group2
            c[&#34;TH&#34;] d[&#34;TT&#34;]
        end
    end
    style group1 fill:#999, text-decoration:line-through
    style a fill:#999, text-decoration:line-through
    style b fill:#999, text-decoration:line-through
    style group2 stroke: #33f, stroke-width: 6px
    style d stroke: #f33, stroke-width: 4px</pre>
<p>There&rsquo;s one outcome ($TT$) out of two possible outcomes ($\{TH, TT\}$). Thus the probability is $1/2$: $P(\{HT, TT\} \mid \{TH, TT\}) = 1/2$.</p>
<p>Tanaka-san says this time the result is what I expected. he says &ldquo;If I just think about what the probability of the second meal is and make that my outcome space, then the probability of the second meal being tonkatsu should just be one-half. Chibany asks Tanaka-san the draw out this outcome space and calculate the probability this way instead. Chibany notes that probability is much more fun when you ask your friends to help you do the hard parts!</p>
<pre class="mermaid align-center ">block-beta
    block:group2
        columns 2
        c[&#34;H&#34;] d[&#34;T&#34;]
    end
    style group2 stroke: #33f, stroke-width: 6px
    style d stroke: #f33, stroke-width: 4px</pre>
<p>Look at that &ndash; It&rsquo;s one half! Chibany prefers learning that there will be at least one tonkatsu because it makes it more likely that he will get a tonkatsu for his second offering.</p>
<p>We saw in one case that conditioning on an event (that there will be one tonkatsu) influenced the probability of another event (that the second offering will be tonkatsu). But in a different case, conditioning on a slighlty differnt event (that the first meal will be a tonkatsu) did not influence the probability of another event (again, that the second offering will be a tonkatsu).</p>
<p>When conditioning on one event $A$ influences the probability of another event $B$, those two events are called <a href="/chapters/intro/index.html#dependence">dependent</a>. This is denoted as $A \not\perp B$. If they do not influence each other they are called independent, which is denoted as $A \perp B$.</p>
<h2 id="marginal-and-joint-probabilities">Marginal and joint probabilities</h2>
<h3 id="chibany-is-sad-marginalization">Chibany is sad (marginalization)</h3>
<p><a href="#R-image-77b81610daef51705f24d7f7ae04aa2c" class="lightbox-link"><img alt="Chibany is sad" class="lazy lightbox figure-image" loading="lazy" src="/images/chibanysad.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-77b81610daef51705f24d7f7ae04aa2c"><img alt="Chibany is sad" class="lazy lightbox lightbox-image" loading="lazy" src="/images/chibanysad.png"></a></p>
<p>The student that normally gives Chibany his second offering is out sick. Now Chibany only gets one offering per day. Chibany lists out the new set of possibilities $\Omega_1 = \{H, T\}$.</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 2
        a[&#34;H(amburger)&#34;] b[&#34;T(onkastu)&#34;]
    end
        style b stroke: #f33, stroke-width:4px</pre>
<p>He notes this is a much sadder set of possibilities. At least the probability of getting Tonkatsu isn&rsquo;t too low! It&rsquo;s one of two possibilities.</p>
<p>Thankfully, on the next day, the student is healthy again and Chibany is back to getting two offerings each day. This changes the set of possibilities back to the original one $\Omega_2 = \{HH,HT, TH, TT \}$. Chibany realizes he can calculate the probability of the first offering being Tonkatsu. Getting his second meal shouldn&rsquo;t influence the chance the first one is Tonkatsu, right? Let&rsquo;s check!</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 2
        a[&#34;H(amburger) H(amburger)&#34;] b[&#34;H(amburger) T(onkastu)&#34;]
        c[&#34;T(onkastu) H(amburger)&#34;] d[&#34;T(onkastu) T(onkastu)&#34;]
    end
    style c stroke: #f33, stroke-width:4px
    style d stroke: #f33, stroke-width:4px</pre>
<p>In this case, he is interested $P(\{TH, TT \}) = 2/4 = 1/2$. Phew!</p>
<p>What happened here? In both cases, we are interested in the same <em>event</em> &ndash; the probability the first meal is a Tonkatsu. In the first case, we did not include the second meal. This is called using <a href="/chapters/intro/index.html#marginal-probability">marginal probability</a>. In the second case, we did include the second meal. This is called using <a href="/chapters/intro/index.html#joint-probability">joint probability</a>. Technically it counts the number of outcomes in the intersection of the different events being considered jointly. This means the number of outcomes that are in all the events under consideration.</p>
<h3 id="the-sum-rule-more-on-marginalization-and-marginal-probabilities">The Sum Rule: More on Marginalization and Marginal Probabilities</h3>
<p>Intuitively, the following two ways of calculating the probability a variable takes a value should give the same answer: (<a href="/chapters/intro/index.html#marginal-probability">marginal probability</a>) list the possible outcomes containing only that variable and count those where it has the specified value, and (2) enumerate the possible outcomes containing that variable and another variable and count all of those where the first variable has the value of interest (<a href="/chapters/intro/index.html#joint-probability">joint probability</a>).</p>
<p>Formally, if we have two random variables $A$ and $B$, the marginal probability of $A$ $P(A)$ is</p>
<p>$P(A) = \sum_{b} P(A, B=b)$.</p>
<p>If you&rsquo;re unfamiliar with the notation $\sum_{b}$, $\sum$ is a fancy way of saying &ldquo;add the following up&rdquo; and the $b$ tells you which values to add up over (in this case, the values $b$ that random variable $B$ could possibly be).</p>
<p>In the last example, $A$ was Chibany&rsquo;s first meal and $B$ was Chibany&rsquo;s second meal. We were interested in whether Chibany&rsquo;s first meal was Tonkatsu or $P(A=T)$. The possible values for $B$ are Hamburger and Tonkatsu or ${H,T }$. What we showed was
$
P(A=T) = \sum_{b} P(A=T,B=b) = P(A=T, B=H) + P(A=T, B=T) = 1/4 + 1/4 = 2/4 = 1/2
$</p>
<h3 id="the-other-definition-of-conditional-probability">The other definition of conditional probability</h3>
<p>Using joint and marginal probabilities, we can define conditional probability in a different manner &ndash; as the ratio of the joint probability to the marginal probability of the conditioned information. Or</p>
<p>$P(A \mid B) = \frac{P(A,B)}{P(B)}$</p>
<p>Note that the probability of $B$ must be greater than zero ($P(B) &gt; 0$). This makes sense to Chibany. How could he be given information that had zero chance of happening?</p>
<p>Chibany is no fan of this other way of calculating conditional probabilities, but he decides to practice using it. He goes back to his favorite example so far &ndash; the one where he had better than a one-half chance of getting two Tonkatsus. In that example, he learned he was going to get at least one Tonkatsu and was interested in finding the probability that there would be two Tonkatsus. So, $A$ is getting a tonkatsu dinner (second meal is tonkatsu) and $B$ is that there is at least one tonkatsu. So $A = \{HT, TT\}$ and $B=\{HT, TH, TT\}$. The intersection or common possibilities in $A$ and $B$ is $\{HT,TT\}$. Remember that there are four possible outcomes in the larger outcome space $\Omega = \{HH,HT,TH,TT\}$ This means $P(A,B) = |\{HT,TT\}/ | \{HH,HT,TH,TT\} = 2/4$. $P(B) = |\{HT,TH,TT\}|/\{HH,HT,TH,TT\} = 3/4$. Putting these together we get
$P(A \mid B) = \frac{P(A,B)}{P(B)} = \frac{2/4}{3/4} = \frac{2}{3}$</p>
<p>Although Chibany is happy to see the same result of it being more likely than not he&rsquo;ll have a second meal of Tonkatsu if he learns he gets at least one Tonkatsu, this felt a lot harder to him than the first way of doing things. It may have felt that way for you too (it does for me!). That&rsquo;s why Chibany wants everyone to know the set-based perspective to probability.</p>
<h2 id="weighted-possibilities">Weighted possibilities</h2>
<h3 id="chibany-tells-students-that-he-likes-tonkatsu-more">Chibany tells students that he likes Tonkatsu more</h3>
<p><a href="#R-image-f5f0a23961e2c70c9175688b273d8fc0" class="lightbox-link"><img alt="Chibany is happy" class="lazy lightbox figure-image" loading="lazy" src="/images/chibanyplain.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f5f0a23961e2c70c9175688b273d8fc0"><img alt="Chibany is happy" class="lazy lightbox lightbox-image" loading="lazy" src="/images/chibanyplain.png"></a></p>
<p>Chibany is happy! He remembered that students love learning. He has important information for them: Chibany likes Tonkatsu more than Hamburgers.</p>
<p>While wondering how to calculate probabilities taking this glorious news into account, Tanaka-san stops by. Tanaka-san lets Chibany know that the students coordinate to ensure that he gets at least one tonkatsu, but try not to make both offerings tonkatsu (that way he doesn&rsquo;t get tired of Tonkatsu). Tanaka-san shares the following chart the students use to guide their daily offerings</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 2
        a[&#34;HH: 4%&#34;] b[&#34;HT: 43%&#34;]
        c[&#34;TH: 43%&#34;] d[&#34;TT: 10%&#34;]
    style b stroke: #f33, stroke-width:4px
    style c stroke: #f33, stroke-width:4px
    style d stroke: #f33, stroke-width:4px
    end</pre>
<p>Chibany is confused at first, but he sticks with the rules he learned. I follow the same procedure as before, but add the weighted versions of each outcome rather than each outcome counting 1 automatically.</p>
<p>So he adds up the outcomes containing Tonkatsu (outlined in red) and divides it by the total amount:
$P(\textrm{Tonkatsu}) = \frac{0.43+0.43+0.10}{0.04+0.43+0.43+0.10} = \frac{0.96}{1}=0.96$</p>
<p>He gets a lot more Tonkatsu &ndash; Tonkatsu 96% of the time. Joyous times!</p>
<p>Practice question:
Can you determine whether the first and second meals are dependent? How would you do that?

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    answer
  </summary>
  <div class="box-content">
<p>
If $A$ and $B$ are random variables encoding Chibany&rsquo;s first meal and second meals, we would want to see whether $P(A=a)$ is different from $P(A =a \mid B=b)$ for any possible $a$ or $b$. Let&rsquo;s consider whether the probability the first meal is Tonkatsu is influenced by the second meal being Tonkatsu. First let&rsquo;s calculate $P(A=T)$. To do, we&rsquo;ll use the sum rule, so $P(A=T) = \sum_b{P(A=T, B= b)} = P(A=T, B=H) + P(A=T, B=T) = 0.43+0.10 = 0.53$. Is this different from the $P(A = T \mid B=T)$? How do we calculate this in the weighted case? The same as before except the $| \Omega|$ is the amount of weight for the conditioned event $B=T$. So,  $P(A=T \mid B=T) = \frac{0.10}{0.43+0.10} = \frac{0.1}{0.53} \approx 0.19$.
  </div>
</details></p>
<h1 id="bayes-theorem-or-bayes-rule">Bayes&rsquo; Theorem or Bayes&rsquo; Rule</h1>
<p><a href="/chapters/intro/index.html#bayes-theorem">Bayes&rsquo; Theorem</a> (Bayes&rsquo; rule) provides a way to update our beliefs in one random variable given information about a different random variable. Let&rsquo;s say we have certain hypotheses about how the world works, which we denote as random variable $H$. Further, we have senses that provide us information. Let&rsquo;s encode the information that we might get from our senses as $D$ (maybe an image from our eyes) and we currently observe $d$ (maybe a picture of tonkatsu).</p>
<p>Bayes Theorem tells us to update our beliefs in hypothesis $h$ being the way the world works after learning $D=d$ in the following manner:</p>
<p>$P(H=h \mid D = d) = \frac{P(D=d\mid H=h) P(H=h)}{P(D=d)}$</p>
<p>where $P(D=d \mid H=h)$ is called the <em>likelihood</em>, whic h is the probability of observing $d$ given $h$ is the true hypothesis for how the world works, $P(H=h)$ is called the <em>prior</em>, which tells us how likely it is that $h$ is the way the world works</p>
<p>We have all the information to prove this! Feel free to skip to the next subsection if you don&rsquo;t care about proofs.</p>
<h2 id="proving-bayes-rule">Proving Bayes&rsquo; rule</h2>
<p>Using the <a href="/chapters/intro/index.html#the-other-definition-of-conditional-probability">other definition of conditional probability</a>, we know that $P(H \mid D) = \frac{P(H,D)}{P(D)}$. If we multiply both sides of the equation by $P(D)$, we get $P(H,D) = P(H \mid D) P(D)$. We can do the same thing but for the opposite way of conditioning (the joint probability can be written in either order and it is the same as it is the common elements of two sets which is the same no matter which order you consider the two sets), so $P(D \mid H) = \frac{P(H,D)}{P(H)}$. We can can solve for $P(H,D)$ in a similar manner: multiply both sides of the equation by $P(H)$ and we get $P(H,D) = P(D \mid H) P(H)$. Putting these together, we can prove Bayes&rsquo; rule:</p>
<p>$P(H \mid D) P(D) = P(H,D) = P(D \mid H) P(H)$
$\Rightarrow P(H \mid D) = \frac{P(H,D)}{P(D)} = \frac{P(D \mid H) P(H)}{P(D)}$</p>
<h2 id="the-taxicab-problem">The Taxicab Problem</h2>
<p><strong>TODO: Add picture with chibany seeing a hit and run with a taxi with fog/smoke</strong></p>
<p>In Chibany&rsquo;s hometown, there are two taxi companies: the Green <i class="fa-fw fas fa-taxi cstyle green"></i> and the Blue <i class="fa-fw fas fa-taxi cstyle blue"></i>. All Green company&rsquo;s taxis are painted green <i class="fa-fw fas fa-taxi cstyle green"></i> and all the Blue company&rsquo;s taxis are painted blue <i class="fa-fw fas fa-taxi cstyle blue"></i>.</p>
<p>85% of the town&rsquo;s taxis work for the Green <i class="fa-fw fas fa-taxi cstyle green"></i> company. So 15% fo the town&rsquo;s taxis work for the <i class="fa-fw fas fa-taxi cstyle blue"></i> company.</p>
<p>Late one foggy evening, Chibany saw a cab perform a hit-and-run (hit another car and leave without providing any information). Chibany saw a <i class="fa-fw fas fa-taxi cstyle blue"></i> taxi!</p>
<p>Chibany is an outstanding citizen and so they go to the police with this information. The police know it was foggy and dark, so it&rsquo;s possible Chibany might not have seen the taxi&rsquo;s color correctly. They test Chibany several times and find that Chibany reports the correct taxi color 80% of the time!</p>
<p>Taking all of this information into account, how likely do you think it is that the cab involved in the hit-and-run was a Blue taxi <i class="fa-fw fas fa-taxi cstyle blue"></i>?</p>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    answer
  </summary>
  <div class="box-content">
<p>
The correct answer is 41%, but most people think it is closer to 60-80%!
  </div>
</details>
<p>This is known as the Taxicab Problem (Kahneman and Tversky, 1972; Bar-Hillel, 1980).</p>
<p>A note: Kahneman and Tversky (and others) use this example (and others) to argue that people are not Bayesian at all! There are a number of replies through the years and it is an ongoing debate. Joe loves discussing it. If interested, please reach out and he would be more than happy to discuss it more.</p>
<h3 id="taxicab-solution-1">Taxicab Solution 1</h3>
<p>One way to solve this is to use the outcome space perspective! Let us assume there are 100 taxis in Chibany&rsquo;s hometown. That means the set of possibilities $\Omega$ has 85 individual Green taxis <i class="fa-fw fas fa-taxi cstyle green"></i> and 15 individual Blue taxis <i class="fa-fw fas fa-taxi cstyle blue"></i></p>
<pre class="mermaid align-center ">block-beta
    block
        columns 10
        g1[&#34;fa:fa-taxi&#34;] g2[&#34;fa:fa-taxi&#34;] g3[&#34;fa:fa-taxi&#34;] g4[&#34;fa:fa-taxi&#34;] g5[&#34;fa:fa-taxi&#34;] g6[&#34;fa:fa-taxi&#34;] g7[&#34;fa:fa-taxi&#34;] g8[&#34;fa:fa-taxi&#34;] g9[&#34;fa:fa-taxi&#34;] g10[&#34;fa:fa-taxi&#34;]
        g11[&#34;fa:fa-taxi&#34;] g12[&#34;fa:fa-taxi&#34;] g13[&#34;fa:fa-taxi&#34;] g14[&#34;fa:fa-taxi&#34;] g15[&#34;fa:fa-taxi&#34;] g16[&#34;fa:fa-taxi&#34;] g17[&#34;fa:fa-taxi&#34;] g18[&#34;fa:fa-taxi&#34;] g19[&#34;fa:fa-taxi&#34;] g20[&#34;fa:fa-taxi&#34;]
        g21[&#34;fa:fa-taxi&#34;] g22[&#34;fa:fa-taxi&#34;] g23[&#34;fa:fa-taxi&#34;] g24[&#34;fa:fa-taxi&#34;] g25[&#34;fa:fa-taxi&#34;] g26[&#34;fa:fa-taxi&#34;] g27[&#34;fa:fa-taxi&#34;] g28[&#34;fa:fa-taxi&#34;] g29[&#34;fa:fa-taxi&#34;] g30[&#34;fa:fa-taxi&#34;]
        g31[&#34;fa:fa-taxi&#34;] g32[&#34;fa:fa-taxi&#34;] g33[&#34;fa:fa-taxi&#34;] g34[&#34;fa:fa-taxi&#34;] g35[&#34;fa:fa-taxi&#34;] g36[&#34;fa:fa-taxi&#34;] g37[&#34;fa:fa-taxi&#34;] g38[&#34;fa:fa-taxi&#34;] g39[&#34;fa:fa-taxi&#34;] g40[&#34;fa:fa-taxi&#34;]   
        g41[&#34;fa:fa-taxi&#34;] g42[&#34;fa:fa-taxi&#34;] g43[&#34;fa:fa-taxi&#34;] g44[&#34;fa:fa-taxi&#34;] g45[&#34;fa:fa-taxi&#34;] g46[&#34;fa:fa-taxi&#34;] g47[&#34;fa:fa-taxi&#34;] g48[&#34;fa:fa-taxi&#34;] g49[&#34;fa:fa-taxi&#34;] g50[&#34;fa:fa-taxi&#34;]
        g51[&#34;fa:fa-taxi&#34;] g52[&#34;fa:fa-taxi&#34;] g53[&#34;fa:fa-taxi&#34;] g54[&#34;fa:fa-taxi&#34;] g55[&#34;fa:fa-taxi&#34;] g56[&#34;fa:fa-taxi&#34;] g57[&#34;fa:fa-taxi&#34;] g58[&#34;fa:fa-taxi&#34;] g59[&#34;fa:fa-taxi&#34;] g60[&#34;fa:fa-taxi&#34;]
        g61[&#34;fa:fa-taxi&#34;] g62[&#34;fa:fa-taxi&#34;] g63[&#34;fa:fa-taxi&#34;] g64[&#34;fa:fa-taxi&#34;] g65[&#34;fa:fa-taxi&#34;] g66[&#34;fa:fa-taxi&#34;] g67[&#34;fa:fa-taxi&#34;] g68[&#34;fa:fa-taxi&#34;] g69[&#34;fa:fa-taxi&#34;] g70[&#34;fa:fa-taxi&#34;]
        g71[&#34;fa:fa-taxi&#34;] g72[&#34;fa:fa-taxi&#34;] g73[&#34;fa:fa-taxi&#34;] g74[&#34;fa:fa-taxi&#34;] g75[&#34;fa:fa-taxi&#34;] g76[&#34;fa:fa-taxi&#34;] g77[&#34;fa:fa-taxi&#34;] g78[&#34;fa:fa-taxi&#34;] g79[&#34;fa:fa-taxi&#34;] g80[&#34;fa:fa-taxi&#34;]  
        g81[&#34;fa:fa-taxi&#34;] g82[&#34;fa:fa-taxi&#34;] g83[&#34;fa:fa-taxi&#34;] g84[&#34;fa:fa-taxi&#34;] g85[&#34;fa:fa-taxi&#34;] b11[&#34;fa:fa-taxi&#34;] b12[&#34;fa:fa-taxi&#34;] b13[&#34;fa:fa-taxi&#34;] b14[&#34;fa:fa-taxi&#34;] b15[&#34;fa:fa-taxi&#34;] 
        b1[&#34;fa:fa-taxi&#34;] b2[&#34;fa:fa-taxi&#34;] b3[&#34;fa:fa-taxi&#34;] b4[&#34;fa:fa-taxi&#34;] b5[&#34;fa:fa-taxi&#34;] b6[&#34;fa:fa-taxi&#34;] b7[&#34;fa:fa-taxi&#34;] b8[&#34;fa:fa-taxi&#34;] b9[&#34;fa:fa-taxi&#34;] b10[&#34;fa:fa-taxi&#34;]


    classDef blueTaxi color: #06f, min-width:22px, font-size:18px
    classDef greenTaxi color: #0d2, min-width:22px, font-size:18px
    class b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14,b15 blueTaxi  
    class g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,g14,g15,g16,g17,g18,g19,g20,g21,g22,g23,g24,g25,g26,g27,g28,g29,g30,g31,g32,g33,g34,g35,g36,g37,g38,g39,g40,g41,g42,g43,g44,g45,g46,g47,g48,g49,g50,g51,g52,g53,g54,g55,g56,g57,g58,g59,g60,g61,g62,g63,g64,g65,g66,g67,g68,g69,g70,g71,g72,g73,g74,g75,g76,g77,g78,g79,g80,g81,g82,g83,g84,g85 greenTaxi
    end</pre>
<p>Now I can make the outcome space include the taxi color and whether Chibany identifies the taxi as Blue in foggy nighttime conditions. As Chibany correctly identifies 80% of the Blue taxis as Blue, ($15 \times 0.80=12$). This means 12 of the Blue taxis are identified as Blue and ($15 \times 0.2 = 3$) 3 are incorrectly as Green. As Chibany <em>incorrectly</em> identifies 20% of the Green taxis as Blue. This means ($85 \times 0.2 = 17$) 17 of the Green taxis are identified as Blue and ($85 \times 0.8=68$) 68 are <em>correctly</em> identified as Green.</p>
<pre class="mermaid align-center ">block-beta
    block
        columns 10
        g1[&#34;fa:fa-taxi&#34;] g2[&#34;fa:fa-taxi&#34;] g3[&#34;fa:fa-taxi&#34;] g4[&#34;fa:fa-taxi&#34;] g5[&#34;fa:fa-taxi&#34;] g6[&#34;fa:fa-taxi&#34;] g7[&#34;fa:fa-taxi&#34;] g8[&#34;fa:fa-taxi&#34;] g9[&#34;fa:fa-taxi&#34;] g10[&#34;fa:fa-taxi&#34;]
        g11[&#34;fa:fa-taxi&#34;] g12[&#34;fa:fa-taxi&#34;] g13[&#34;fa:fa-taxi&#34;] g14[&#34;fa:fa-taxi&#34;] g15[&#34;fa:fa-taxi&#34;] g16[&#34;fa:fa-taxi&#34;] g17[&#34;fa:fa-taxi&#34;] g18[&#34;fa:fa-taxi&#34;] g19[&#34;fa:fa-taxi&#34;] g20[&#34;fa:fa-taxi&#34;]
        g21[&#34;fa:fa-taxi&#34;] g22[&#34;fa:fa-taxi&#34;] g23[&#34;fa:fa-taxi&#34;] g24[&#34;fa:fa-taxi&#34;] g25[&#34;fa:fa-taxi&#34;] g26[&#34;fa:fa-taxi&#34;] g27[&#34;fa:fa-taxi&#34;] g28[&#34;fa:fa-taxi&#34;] g29[&#34;fa:fa-taxi&#34;] g30[&#34;fa:fa-taxi&#34;]
        g31[&#34;fa:fa-taxi&#34;] g32[&#34;fa:fa-taxi&#34;] g33[&#34;fa:fa-taxi&#34;] g34[&#34;fa:fa-taxi&#34;] g35[&#34;fa:fa-taxi&#34;] g36[&#34;fa:fa-taxi&#34;] g37[&#34;fa:fa-taxi&#34;] g38[&#34;fa:fa-taxi&#34;] g39[&#34;fa:fa-taxi&#34;] g40[&#34;fa:fa-taxi&#34;]   
        g41[&#34;fa:fa-taxi&#34;] g42[&#34;fa:fa-taxi&#34;] g43[&#34;fa:fa-taxi&#34;] g44[&#34;fa:fa-taxi&#34;] g45[&#34;fa:fa-taxi&#34;] g46[&#34;fa:fa-taxi&#34;] g47[&#34;fa:fa-taxi&#34;] g48[&#34;fa:fa-taxi&#34;] g49[&#34;fa:fa-taxi&#34;] g50[&#34;fa:fa-taxi&#34;]
        g51[&#34;fa:fa-taxi&#34;] g52[&#34;fa:fa-taxi&#34;] g53[&#34;fa:fa-taxi&#34;] g54[&#34;fa:fa-taxi&#34;] g55[&#34;fa:fa-taxi&#34;] g56[&#34;fa:fa-taxi&#34;] g57[&#34;fa:fa-taxi&#34;] g58[&#34;fa:fa-taxi&#34;] g59[&#34;fa:fa-taxi&#34;] g60[&#34;fa:fa-taxi&#34;]
        g61[&#34;fa:fa-taxi&#34;] g62[&#34;fa:fa-taxi&#34;] g63[&#34;fa:fa-taxi&#34;] g64[&#34;fa:fa-taxi&#34;] g65[&#34;fa:fa-taxi&#34;] g66[&#34;fa:fa-taxi&#34;] g67[&#34;fa:fa-taxi&#34;] g68[&#34;fa:fa-taxi&#34;] g69[&#34;fa:fa-taxi&#34;] g70[&#34;fa:fa-taxi&#34;]
        g71[&#34;fa:fa-taxi&#34;] g72[&#34;fa:fa-taxi&#34;] g73[&#34;fa:fa-taxi&#34;] g74[&#34;fa:fa-taxi&#34;] g75[&#34;fa:fa-taxi&#34;] g76[&#34;fa:fa-taxi&#34;] g77[&#34;fa:fa-taxi&#34;] g78[&#34;fa:fa-taxi&#34;] g79[&#34;fa:fa-taxi&#34;] g80[&#34;fa:fa-taxi&#34;]  
        g81[&#34;fa:fa-taxi&#34;] g82[&#34;fa:fa-taxi&#34;] g83[&#34;fa:fa-taxi&#34;] g84[&#34;fa:fa-taxi&#34;] g85[&#34;fa:fa-taxi&#34;] b11[&#34;fa:fa-taxi&#34;] b12[&#34;fa:fa-taxi&#34;] b13[&#34;fa:fa-taxi&#34;] b14[&#34;fa:fa-taxi&#34;] b15[&#34;fa:fa-taxi&#34;] 
        b1[&#34;fa:fa-taxi&#34;] b2[&#34;fa:fa-taxi&#34;] b3[&#34;fa:fa-taxi&#34;] b4[&#34;fa:fa-taxi&#34;] b5[&#34;fa:fa-taxi&#34;] b6[&#34;fa:fa-taxi&#34;] b7[&#34;fa:fa-taxi&#34;] b8[&#34;fa:fa-taxi&#34;] b9[&#34;fa:fa-taxi&#34;] b10[&#34;fa:fa-taxi&#34;]


    classDef blueTaxi color: #06f, min-width:22px, font-size:18px, stroke: #f33, stroke-width:2px
    classDef blueGrayTaxi color: #028, min-width:22px, font-size:18px
    classDef greenTaxi color: #0d2, min-width:22px, font-size:18px, stroke: #f33, stroke-width:2px
    classDef greenGrayTaxi color: #051, min-width:22px, font-size:18px
    class b1,b2,b3,b4,b5,b6,b7,b11,b12,b13,b14,b15 blueTaxi  
    class b8,b9,b10 blueGrayTaxi
    class g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,g14,g15,g16,g17 greenTaxi
    class g18,g19,g20,g21,g22,g23,g24,g25,g26,g27,g28,g29,g30,g31,g32,g33,g34,g35,g36,g37,g38,g39,g40,g41,g42,g43,g44,g45,g46,g47,g48,g49,g50,g51,g52,g53,g54,g55,g56,g57,g58,g59,g60,g61,g62,g63,g64,g65,g66,g67,g68,g69,g70,g71,g72,g73,g74,g75,g76,g77,g78,g79,g80,g81,g82,g83,g84,g85 greenGrayTaxi
    end</pre>
<p>The brightly colored taxis that are outlined in red are those that Chibany reports as Blue in the difficult viewing conditions. We can already see there are more Green <i class="fa-fw fas fa-taxi cstyle green"></i> taxis tha Blue <i class="fa-fw fas fa-taxi cstyle blue"></i>, so it is still more probable that the taxi involved in the hit-and-run was Green. We can get the exact probability that it was a Blue taxi <i class="fa-fw fas fa-taxi cstyle blue"></i> by the same counting rule as before. There are 12 <i class="fa-fw fas fa-taxi cstyle blue"></i> Blue taxis and 17 <i class="fa-fw fas fa-taxi cstyle green"></i> Green taxis. So, the probability that it was a blue taxi given Chibany reports it as Blue is $12/(12+17)=12/29 \approx 0.41$.</p>
<h3 id="taxicab-solution-2">Taxicab Solution 2</h3>
<p>We can also solve this without counting in a sample space following the rules of probability theory as described before. Let $X$ be the actual color of the taxi involved in the hit-and-run and $W$ be the color reported by Chibany. Based on the percentage of Blue <i class="fa-fw fas fa-taxi cstyle blue"></i> and Green <i class="fa-fw fas fa-taxi cstyle green"></i> taxis in the city, we know that $P(X=G) = 0.85$ and $P(X=B)=0.15$. We also know that Chibany is accurate 80% of the time. So, $P(W = B \mid X = B) = 0.8$ and $P(W=G \mid X=G)=0.8$. This also means Chibany is inaccurate 20% fo the tim$e: $P(W = B \mid X=G)=0.2$ and $P(W=G \mid X=B)=0.2$.</p>
<p>Chibany said the taxi is Blue and given this, how likely it is that the taxi is Blue. So, we&rsquo;re interested in $P(X=B \mid W=B)$ We can solve this using Bayes&rsquo; rule and the sum rule.</p>
<p>$P(X=B \mid W=B) = \frac{P(W =B \mid X=B) P(X=B)}{P(W=B)}$</p>
<p>$P(X=B \mid W=B) = \frac{P(W =B \mid X=B) P(X=B)}{\sum_c{P(W=B,X=c)}}$</p>
<p>$P(X=B \mid W=B) = \frac{P(W =B \mid X=B) P(X=B)}{\sum_c{P(W=B \mid X=c)P(X=c)}}$</p>
<p>$P(X=B \mid W=B) = \frac{P(W =B \mid X=B) P(X=B)}{P(W=B \mid X=B)P(X=B) + P(W=B \mid X=G)P(X=G)}$</p>
<p>$P(X=B \mid W=B) = \frac{0.8 \times 0.15 }{0.8 \times 0.15 + 0.2 \times 0.85} = \frac{0.12}{0.12+0.17} = \frac{0.12}{0.29} \approx 0.41$</p>
<h3 id="why-learn-the-set-based-perspective-to-probability-theory">Why Learn the Set-Based Perspective to Probability Theory?</h3>
<p>If we can solve probability problems via symbol manipulation, why is learning the set-based perspective to probability theory?</p>
<p>Here are some reasons:</p>
<ol>
<li>As variables become more complex, explicitly solving problems becomes infeasible. Thinking through how to count is strong starting point for a <a href="/chapters/intro/index.html#generative-process">generative process</a> perspective, which discusses how outcomes are produced according to computer programs with random choices. These define proabilistic models! <a href="/chapters/intro/index.html#probabilistic-computing">Probabilistic computing</a> are programming languages for specifying probabilistic models and built to calculate different probabilities according to this model in an efficient manner. We will build to exploring how to do this over the next few tutorials.</li>
<li>Many probability novices find the distinction between joint and conditional probabilities confusing and unintuitive. From the set-based perspective, their difference is clear. Joint probabilities count in the outcome space where multiple possible outcomes are being simultaneously. Conditional probabilities change the outcome space to be whatever is consistent with the conditioned information and then count in that new space.</li>
<li>It forces you to think about how events and outcomes are represented. This can be obsfucated at times when thinking about probabilities from the rule-based perspective.</li>
<li>They are formally equivalent.</li>
<li>It is fun.</li>
<li>It connects combinatorics and probability theory.</li>
<li>It makes Chibany happy.</li>
</ol>
<h3 id="transfer-additional-practice-questions">Transfer additional practice questions</h3>
<ul>
<li>
<p>Example with rare disease and not too diagnostic test.</p>
</li>
<li>
<p>Example with organic fruit and made at a local place</p>
</li>
</ul>
<h1 id="glossary">Glossary</h1>
<h3 id="set">set</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    set
  </summary>
  <div class="box-content">
<p>
A <em>set</em> is a collection of elements or members. Sets are defined by the elements they do or do not contain. The elements are listed with commas between them and &ldquo;$\{$&rdquo; denotes the start of a set and &ldquo;$\}$&rdquo; the end of a set. Note that the elements of a set are unique.
  </div>
</details>
<h3 id="event">event</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    event
  </summary>
  <div class="box-content">
<p>
An <em>event</em> is a set that is none, some, or all of the possible outcomes.
  </div>
</details>
<h3 id="cardinality">cardinality</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    cardinality
  </summary>
  <div class="box-content">
<p>
The <em>cardinality</em> or <em>size</em> of a set is the number of elements it contains. If $A = \{H, T\}$, then the cardinality $A$ is $|A|=2$.
  </div>
</details>
<h3 id="probability">probability</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    probability
  </summary>
  <div class="box-content">
<p>
The <em>probability</em> of an event $A$ relative to an outcome space $\Omega$ is the ratio of their sizes or $\frac{|A|}{|\Omega|}
  </div>
</details>
<h3 id="random-variable">random variable</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    random variable
  </summary>
  <div class="box-content">
<p>
A <em>random variable</em> is a function that maps from the set of possible outcomes to some set or space. The output or range of the function could be the set of outcomes again, a whole number based on the outcome (e.g., counting the number of Tonkatsu), or something more complex (e.g., the world&rsquo;s friendship matrix, an 8-billion by 8-billion, binary matrix where $N$ where $N_{1,100}=1$ if person 1 is friends with person 100). Technically the output must be <em>measurable</em>. You shouldn&rsquo;t worry about that distinction unless your random variable&rsquo;s output gets really, really big (like continuous). We&rsquo;ll talk more about probabilities over continuous random variables later.
  </div>
</details>
<h3 id="conditional-probability">conditional probability</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    conditional probability
  </summary>
  <div class="box-content">
<p>
The <em>conditional probability</em> is the probability of an event conditioned on knowledge of another event. Conditioning on an event means that the possible outcomes in that event form the set of possibilities or outcome space. We then calculate probabilities as normal within that <em>restricted</em> outcome space. Formally, this is written as $P(A \mid B) = \frac{|A|}{|B|}$, where everything to the left of the $\mid$ is what we&rsquo;re interested in knowing the probability of and everything to the right of the $\mid$ is what we know to be true.
  </div>
</details>
<h3 id="dependence">dependence</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    dependence
  </summary>
  <div class="box-content">
<p>
When knowing the outcome of one random variable or event influences the probability of another, those variables or events are called <em>dependent</em>. This is denoted as $A \not\perp B$. When they do not influence each other, they are called <em>independent</em>. This is denoted as $A \perp B$.
  </div>
</details>
<h3 id="marginal-probability">marginal probability</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    marginal probability
  </summary>
  <div class="box-content">
<p>
A <em>marginal probability</em> is the probability of a random variable that has been calculated by summing over the possible values of one or more other random variables.
  </div>
</details>
<h3 id="joint-probability">joint probability</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    joint probability
  </summary>
  <div class="box-content">
<p>
The <em>joint probability</em> is the probability of all considered events. This corresponds to the intersection of the events.
  </div>
</details>
<h3 id="bayes-theorem">Bayes theorem</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    Bayes Theorem
  </summary>
  <div class="box-content">
<em>Bayes Theorem</em> is a rule for reversing the order that variables are conditioned &ndash; how to go from $P(A \mid B)$ to $P(B \mid A)$
  </div>
</details>
<h3 id="generative-process">generative process</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    generative process
  </summary>
  <div class="box-content">
<p>
A <em>generative process</em> defines the probabilities for possible outcomes according to an algorithm with random choices.
  </div>
</details>
<h3 id="probabilistic-computing">probabilistic computing</h3>

<details class=" box cstyle notices transparent expand">
  <summary class="box-label">
    <i class="expander-icon fa-fw fas fa-chevron-right"></i> 
    probabilistic computing
  </summary>
  <div class="box-content">
<em>Probabilistic computing</em> is a programming language for specifying probabilistic models and built to calculate different probabilities according to this model in an efficient manner
  </div>
</details>
<h1 id="acknowledgements">Acknowledgements</h1>
<p>Written by Joe Austerweil. Thank you to Kyana Burhite and the many students who took Human and Machine Learning over the years who have provided invaluable feedback on an early draft of this tutorial. Further thanks to the Japan Probabilistic Computing Consortium Association (<a href="https://jpcca.org/" rel="external" target="_blank">JPCCA</a>) for funding my ability to polish and publish this tutorial series.</p>
<p>Please reach out to Joe via email if you have any constructive feedback (anything from X could be more clear or this is a great resource I will share with my class).</p>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Sep 25, 2025
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/index.html">
            <div class="logo-title">Narrative Introduction to Probabilistic Computing</div>
          </a>
        </div>
        <search><form action="/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/index.html"><a class="padding" href="/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="parent " data-nav-id="/chapters/index.html"><a class="padding" href="/chapters/index.html">Chapters</a><ul id="R-subsections-aa0422005ecdb4831dd25398b858a255" class="collapsible-menu">
            <li class="active " data-nav-id="/chapters/intro/index.html"><a class="padding" href="/chapters/intro/index.html">A Narrative Introduction to Probability</a></li>
            <li class="" data-nav-id="/chapters/intro2/index.html"><a class="padding" href="/chapters/intro2/index.html">Intro2</a></li></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script>
      window.MathJax = Object.assign( window.MathJax || {}, {
        tex: {
          inlineMath:  [['\\(', '\\)'], ['$',  '$']],  
          displayMath: [['\\[', '\\]'], ['$$', '$$']], 
        },
        options: {
          enableMenu: false 
        }
      }, JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/js/mathjax/tex-mml-chtml.js?1758795679"></script>
    <script src="/js/js-yaml/js-yaml.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-color.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-dispatch.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-drag.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-ease.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-interpolate.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-selection.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-timer.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-transition.min.js?1758795679" defer></script>
    <script src="/js/d3/d3-zoom.min.js?1758795679" defer></script>
    <script src="/js/mermaid/mermaid.min.js?1758795679" defer></script>
    <script>
      window.relearn.themeUseMermaid = JSON.parse("{}");
    </script>
    <script src="/js/clipboard/clipboard.min.js?1758795679" defer></script>
    <script src="/js/perfect-scrollbar/perfect-scrollbar.min.js?1758795679" defer></script>
    <script src="/js/theme.js?1758795679" defer></script>
  </body>
</html>
